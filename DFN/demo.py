#!/usr/bin/env python3
"""
DFN Inference Demo - DemostraciÃ³n de DFN Pura Entrenado

Script para probar y demostrar el modelo DFN Pura despuÃ©s del entrenamiento.
Incluye generaciÃ³n continua, anÃ¡lisis de entidades, y comparaciÃ³n de rendimiento.
"""

import sys
import os
import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
import gc

# Add project root to path for imports
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from dfn import DFNPureModel
from dfn.utils import print_gpu_memory_info, get_model_memory_footprint, print_memory_summary


def load_trained_model(checkpoint_path, config=None):
    """Loads trained model from checkpoint"""
    print(f"ğŸ“ Loading model from: {checkpoint_path}")
    if config is None:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        config = checkpoint['config']
        print("   âœ… Configuration loaded from checkpoint")
    else:
        print("   âœ… Using provided configuration")
    model = DFNPureModel(**config)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    if checkpoint_path:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        print("   âœ… Model weights loaded")
        print(f"   ğŸ“Š Epoch: {checkpoint['epoch']}")
        print(f"   ğŸ“Š Loss: {checkpoint['loss']:.4f}")
    model.eval()
    if torch.cuda.is_available():
        print(f"   ğŸ® Initial Allocated VRAM: {torch.cuda.memory_allocated() / 1024 / 1024:.2f}MB")
    return model, device, config


def continuous_generation_demo(model, device, config):
    """Continuous generation demonstration"""
    print("\nğŸ¨ CONTINUOUS GENERATION")
    print("=" * 30)
    seed_length = 20
    seed_sequence = torch.randn(1, seed_length, config['input_dim']).to(device)
    print(f"ğŸ“¥ Seed sequence: {seed_sequence.shape}")
    with torch.no_grad():
        generated_field = model(seed_sequence)
        print(f"ğŸ“¤ Generated field: {generated_field.shape}")
        generated_sequence = generated_field.mean(dim=1)
        print(f"ğŸ“„ Generated sequence: {generated_sequence.shape}")
        field_variance = generated_field.var().item()
        sequence_variance = generated_sequence.var().item()
        print(f"   ğŸ“Š Field variance: {field_variance:.6f}")
        print(f"   ğŸ“Š Sequence variance: {sequence_variance:.6f}")
        print(f"   âœ… {'Continuous field generated' if field_variance > 0.01 else 'Low variation field'}")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        allocated = torch.cuda.memory_allocated() / 1024 / 1024
        print(f"   â™»ï¸ After cleanup: Allocated VRAM: {allocated:.2f}MB")


def entity_analysis_demo(model, device, config):
    """Extracted entity analysis"""
    print("\nğŸ” ENTITY ANALYSIS")
    print("=" * 30)
    test_cases = [
        ("Uniform", torch.randn(2, 50, config['input_dim']) * 0.5),
        ("Concentrated", torch.randn(2, 50, config['input_dim'])),
        ("Sequential", torch.randn(2, 50, config['input_dim']))
    ]
    test_cases[2][1][:, 25:, :] *= 3.0  # More information in the second half
    for name, data in test_cases:
        data = data.to(device)
        print(f"\nğŸ“Š Pattern: {name}")
        with torch.no_grad():
            positions, states, weights = model.encode(data)
            print(f"   ğŸ“ Positions: {positions.shape}")
            print(f"   ğŸ§  States: {states.shape}")
            print(f"   âš–ï¸ Weights: {weights.shape}")
            weight_mean = weights.mean().item()
            weight_std = weights.std().item()
            print(f"   ğŸ“ˆ Weight mean: {weight_mean:.6f}")
            print(f"   ğŸ“‰ Weight std: {weight_std:.6f}")
            pos_mean = positions.mean(dim=1)
            pos_std = positions.std().item()
            print(f"   ğŸ¯ Position spread: {pos_std:.4f}")
            if weight_std > 0.001:
                print("   âœ… Entities detect content variation")
            else:
                print("   âš ï¸ Entities with uniform distribution")
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            allocated = torch.cuda.memory_allocated() / 1024 / 1024
            print(f"   â™»ï¸ After cleanup: Allocated VRAM: {allocated:.2f}MB")


def performance_benchmark(model, device, config):
    """Performance benchmark"""
    print("\nâš¡ PERFORMANCE BENCHMARK")
    print("=" * 30)
    test_lengths = [32, 64, 128, 256, 512, 1024 * 200, 1024 * 250]
    print("ğŸ“ Scalability test:")
    for length in test_lengths:
        x = torch.randn(2, length, config['input_dim']).to(device)
        if torch.cuda.is_available():
            before_alloc = torch.cuda.memory_allocated() / 1024 / 1024
            torch.cuda.synchronize()
        else:
            before_alloc = 0
        start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else time.time()
        end_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
        if torch.cuda.is_available():
            start_time.record()
        else:
            start_time = time.time()
        with torch.no_grad():
            output = model(x)
        if torch.cuda.is_available():
            end_time.record()
            torch.cuda.synchronize()
            elapsed = start_time.elapsed_time(end_time) / 1000  # seconds
        else:
            elapsed = time.time() - start_time
        if torch.cuda.is_available():
            after_alloc = torch.cuda.memory_allocated() / 1024 / 1024
            delta_alloc = after_alloc - before_alloc
        else:
            after_alloc = delta_alloc = 0
        print(f"   Length {length:4d}: {elapsed:.3f}s, Current VRAM: {after_alloc:.1f}MB, Î”Alloc: {delta_alloc:.1f}MB")
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    print("\nğŸ¯ Consistency of entities:")
    x = torch.randn(1, 100, config['input_dim']).to(device)
    with torch.no_grad():
        positions, states, weights = model.encode(x)
        num_entities = states.shape[1]
    print(f"   Extracted entities: {num_entities} (constant)")
    print(f"   âœ… No sequence limit - works with any length")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        allocated = torch.cuda.memory_allocated() / 1024 / 1024
        print(f"   â™»ï¸ After cleanup: Allocated VRAM: {allocated:.2f}MB")


def memory_analysis(model, device, config):
    """Complete memory analysis"""
    print("\nğŸ’¾ MEMORY ANALYSIS")
    print("=" * 30)
    memory_info = get_model_memory_footprint(model)
    print("ğŸ“Š Model Memory Breakdown:")
    print(f"   Parameters: {memory_info['parameters_mb']:.1f}MB")
    print(f"   Buffers: {memory_info['buffers_mb']:.1f}MB")
    print(f"   Total: {memory_info['total_mb']:.1f}MB")
    if torch.cuda.is_available():
        allocated_vram = torch.cuda.memory_allocated() / 1024 / 1024
        print(f"ğŸ® Current GPU Allocated VRAM: {allocated_vram:.2f}MB")
    else:
        print("ğŸ® GPU not available.")
    print("\nğŸ”¢ Parameters by Component:")
    total_params = 0
    for name, module in model.named_modules():
        if len(list(module.children())) == 0:
            params = sum(p.numel() for p in module.parameters())
            if params > 0:
                total_params += params
                print(f"   {name}: {params:,} ({params/sum(p.numel() for p in model.parameters())*100:.1f}%)")
    print(f"   Total: {total_params:,}")


def main():
    """Main demonstration function"""
    print("ğŸ­ DFN Pure Inference Demo")
    print("=" * 50)
    print("Script to test trained model")
    print()
    checkpoint_dir = "checkpoints"
    if os.path.exists(checkpoint_dir):
        checkpoints = list(Path(checkpoint_dir).glob("dfn_pure_*.pt"))
        if checkpoints:
            checkpoints.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            latest_checkpoint = checkpoints[0]
            print(f"ğŸ“ Checkpoint found: {latest_checkpoint}")
        else:
            print("âŒ No checkpoints found")
            print("ğŸ’¡ Run train.py first to create a trained model")
            return
    else:
        print(f"âŒ Directory '{checkpoint_dir}' does not exist")
        print("ğŸ’¡ Run train.py first to create a trained model")
        return
    model, device, config = load_trained_model(latest_checkpoint)
    try:
        continuous_generation_demo(model, device, config)
        entity_analysis_demo(model, device, config)
        performance_benchmark(model, device, config)
        memory_analysis(model, device, config)
        print("\nğŸ† DEMONSTRATION COMPLETE!")
        print("=" * 50)
        print("âœ… Pure DFN functioning correctly")
        print("âœ… Continuous generation active")
        print("âœ… Entity analysis functional")
        print("âœ… Scalability verified")
        print(f"âœ… Model saved in: {latest_checkpoint}")
    except Exception as e:
        print(f"\nâŒ Error during demonstration: {e}")
        print("ğŸ’¡ Verify that the model is trained correctly")


if __name__ == "__main__":
    main()
